# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Nova Lang is a compiler written in PureScript that compiles a PureScript-like language to Elixir. The project implements a full compiler pipeline: tokenization → parsing → type checking → code generation.

**Bootstrapping Goal:** We are bootstrapping a self-hosted version of the compiler that will run on the BEAM VM. The PureScript compiler in `src/` compiles itself to Elixir, producing the output in `nova_lang/`. 

Selfhosting has been achieved, now we are removing hardcoded stuff, generalizing existing code and refining it.

## Directory Structure

- `src/Nova/Compiler/` - **Source**: The PureScript compiler source code
- `nova_lang/` - **Compiled Output**: Elixir code generated by compiling the PureScript source
- `scripts/regenerate.js` - Script to recompile PureScript source to Elixir output
- `test/` - PureScript tests for the compiler

## Build Commands

```bash
npx spago build                    # Build PureScript compiler
npx spago test                     # Run PureScript tests
node scripts/regenerate.js         # Recompile source to Elixir (regenerate nova_lang/)
cd nova_lang && mix test           # Run tests on compiled Elixir output
```

## Compiler Architecture

The compiler lives in `src/Nova/Compiler/` with these core modules:

- **Tokenizer.purs** - Lexical analysis producing tokens with position info
- **Parser.purs** - Recursive descent parser producing AST from tokens
- **Ast.purs** - AST type definitions (Module, Declaration, Expr, Pattern, TypeExpr)
- **Types.purs** - Internal type representation for type checking (TVar, TCon, TyRecord, substitutions)
- **TypeChecker.purs** - Hindley-Milner type inference (Algorithm W) with constraints
- **Unify.purs** - Robinson unification algorithm for types
- **CodeGen.purs** - Elixir code generation from typed AST
- **Dependencies.purs** - Dependency extraction and graph management

### Key Types

The AST distinguishes between:
- `Declaration` - Top-level items (functions, data types, newtypes, type classes, imports, infix)
- `Expr` - Expressions (literals, variables, application, lambdas, case, do, let)
- `Pattern` - Pattern matching (variables, constructors, records, lists)
- `TypeExpr` - Surface syntax types (what user writes)
- `Type` - Internal type representation (used by type checker)

### Parser Design

The parser uses explicit token manipulation with helpers like `skipNewlines`, `expectKeyword`, `expectDelimiter`. Parse results are `Either String (Tuple a (Array Token))`.

## Development Workflow

After making changes to the PureScript source:

1. Build: `npx spago build`
2. Test PureScript: `npx spago test`
3. Regenerate Elixir: `node scripts/regenerate.js`
4. Test compiled output: `cd nova_lang && mix test`

**Important:** After every round of changes, verify the compiler can still compile its own source code by running `node scripts/regenerate.js`.

## Current Status

### Working
- Tokenization (complete)
- Parsing
- Type inference (Hindley-Milner with basic type class support)
- Code generation to Elixir (functions, data types, newtypes, case expressions, do-notation)
- Self-compilation

### In Progress
- dehardcoding and generalizing leftovers scafolding on the compiler core modules

## Self-Hosting Verification (CRITICAL)

**The ultimate test for correctness is byte-for-byte identical output.** When the compiled Elixir compiler processes its own PureScript source, it must produce **exactly the same** Elixir code as the PureScript compiler produces.

Run the self-hosting test:
```bash
cd nova_lang && mix run test_self_host_all.exs
```

**Goal:** All modules must show "EXACT MATCH" or at minimum "whitespace diff only". Any semantic difference in generated code indicates a bug in the Elixir compiler.

## Test Structure

- `test/` - PureScript tests (parser tests in `test/parser/`)
- `nova_lang/test/` - Elixir tests for compiled output (namespace service, code generation)

## Parser/Tokenizer Development Guidelines

When fixing bugs in the Parser or Tokenizer:

1. **Create a test first**: For each parser problem found, add a test case to `test/parser/` that reproduces the issue
2. **Run full parser tests**: After any modification to Parser.purs or Tokenizer.purs, run `npx spago test` to ensure no regressions
3. **Test the fix**: Verify the new test passes after the fix

## MCP Interface (AI Agent Dev Environment)

The Nova MCP server (`nova mcp`) exposes the compiler as a service for AI agents to programmatically write, validate, and compile Nova code. This is NOT an LSP - it's a development environment designed for AI agents.

**Key Concepts:**
- **Namespaces**: In-memory workspaces for organizing code (not tied to files)
- **Declarations**: Functions, types, aliases added incrementally to namespaces
- **Immediate feedback**: Type check after each change, get structured errors
- **On-demand compilation**: Generate Elixir when ready

**Workflow for AI agents:**
1. Create a namespace: `create_namespace {name: "MyApp"}`
2. Add declarations one at a time: `add_declaration {namespace: "MyApp", source: "add x y = x + y"}`
3. Validate to check types: `validate_namespace {namespace: "MyApp"}`
4. Fix any errors based on structured feedback
5. Compile when ready: `compile_namespace {namespace: "MyApp"}`

**Compiler core operations:**
- `load_compiler_core` - Load all 8 compiler modules into namespaces
- `compile_compiler` - Compile all modules with dependencies
- `validate_compiler` - Type check all compiler modules

**Available MCP tools:** create_namespace, delete_namespace, list_namespaces, add_declaration, update_declaration, remove_declaration, list_declarations, get_declaration, validate_namespace, get_type, get_diagnostics, get_completions, add_import, list_imports, load_file, compile_file, compile_namespace, load_compiler_core, compile_compiler, validate_compiler

## Development Philosophy

**Root Cause Analysis Over Quick Fixes:** Since this is a compiler project, always investigate and fix the root cause of problems rather than applying band-aid patches or workarounds. For example:
- If generated code has issues, fix the code generator (CodeGen.purs) rather than patching the output
- If tests fail due to missing runtime functions, add proper implementations rather than test-specific hacks
- Time is not a constraint - thoroughness and correctness are more important than speed

**No Workarounds in Test Runners:** Test infrastructure should execute generated code as-is. Never add string replacements, regex patches, or injected code to make tests pass. If tests fail, the fix belongs in the compiler source.

each warning is a waste of tokens, which means a compounding waste of money
